{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import pyautogui\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "video_name = 'test_video.mp4'\n",
    "tenhon_name = 'tenhon.png'\n",
    "tenhon_name2 = 'tenhon2.png'\n",
    "global rc\n",
    "rc = 200\n",
    "global red\n",
    "red = (0,0,255)\n",
    "global green\n",
    "green = (0,255,0)\n",
    "global blue\n",
    "blue = (255,0,0)\n",
    "global marking\n",
    "marking = True\n",
    "showing = True\n",
    "zsize = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_range(str):\n",
    "    \n",
    "    if str == \"blue\":\n",
    "        l = np.array([rc,0,0])\n",
    "        \n",
    "    elif str == \"green\":\n",
    "        l = np.array([0, rc, 0])\n",
    "    elif str == \"red\":\n",
    "        l = np.array([0, 0, rc])\n",
    "\n",
    "    u = np.array([255,255,255])\n",
    "\n",
    "    return l, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img):\n",
    "    cv.imshow(\"title\", img)\n",
    "        \n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        cv.destroyAllWindows()\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    cv.imshow(\"title\", img)\n",
    "    cv.waitKey()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwhile True:\\n\\n    screenshot = pyautogui.screenshot()\\n    screenshot = np.array(screenshot)\\n    screenshot = cv.cvtColor(screenshot, cv.COLOR_RGB2BGR)\\n    cv.imshow('', screenshot)\\n\\n    if cv.waitKey(1) == ord('q'):\\n        cv.destroyAllWindows()\\n        break\\n\\nprint('Done')\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "while True:\n",
    "\n",
    "    screenshot = pyautogui.screenshot()\n",
    "    screenshot = np.array(screenshot)\n",
    "    screenshot = cv.cvtColor(screenshot, cv.COLOR_RGB2BGR)\n",
    "    cv.imshow('', screenshot)\n",
    "\n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        cv.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "print('Done')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_fliter(img, string):\n",
    "    l, u = color_range(string)\n",
    "    mask = cv.inRange(img, l, u)\n",
    "    img = cv.bitwise_or(img, img, mask=mask)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contours(img):\n",
    "\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    im_thresh = cv.adaptiveThreshold(gray, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 21, 0)\n",
    "\n",
    "    return cv.findContours(im_thresh, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rectangle(img, x, y, w, h):\n",
    "    cv.rectangle(img, (x,y), (x+w, y+h), green, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text(img, x, y, string):\n",
    "    cv.putText(img, string, (x, y), cv.FONT_HERSHEY_PLAIN, 1, red, 1, cv.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(path+video_name)\n",
    "\n",
    "while (cap.isOpened()):\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    mask_img = color_fliter(frame, \"blue\")\n",
    "    \n",
    "    contour, hierarchy = find_contours(mask_img)\n",
    "\n",
    "    contour_list = list()\n",
    "    \n",
    "    for pos in range(len(contour)):\n",
    "        \n",
    "        x, y, w, h = cv.boundingRect(contour[pos])\n",
    "\n",
    "        if ((w > 20 and w < 100 and h > 40 and h < 150) or (h > 20 and h < 100 and w > 40 and w < 150)):\n",
    "\n",
    "            contour_list.append(contour[pos])\n",
    "\n",
    "            if marking:\n",
    "\n",
    "                draw_rectangle(frame)\n",
    "        \n",
    "    isBreak = show(frame)\n",
    "        \n",
    "    if isBreak:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'cv::inRange'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32me:\\opencv_test\\test.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m [tenhon_name, tenhon_name2]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     solimg \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39mimread(path \u001b[39m+\u001b[39m tenhon_name2)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     mask_img \u001b[39m=\u001b[39m color_fliter(solimg, \u001b[39m\"\u001b[39;49m\u001b[39mblue\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     contour, hierarchy \u001b[39m=\u001b[39m find_contours(mask_img)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     contour_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n",
      "\u001b[1;32me:\\opencv_test\\test.ipynb Cell 11\u001b[0m in \u001b[0;36mcolor_fliter\u001b[1;34m(img, string)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcolor_fliter\u001b[39m(img, string):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     l, u \u001b[39m=\u001b[39m color_range(string)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     mask \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39;49minRange(img, l, u)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     img \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39mbitwise_or(img, img, mask\u001b[39m=\u001b[39mmask)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'cv::inRange'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name in [tenhon_name, tenhon_name2]:\n",
    "    \n",
    "    solimg = cv.imread(path + tenhon_name2)\n",
    "\n",
    "    mask_img = color_fliter(solimg, \"blue\")\n",
    "\n",
    "    contour, hierarchy = find_contours(mask_img)\n",
    "\n",
    "    contour_list = list()\n",
    "\n",
    "    for pos in range(len(contour)):\n",
    "        \n",
    "        x, y, w, h = cv.boundingRect(contour[pos])\n",
    "\n",
    "        if ((w > 20 and w < 100 and h > 40 and h < 150) or (h > 20 and h < 100 and w > 40 and w < 150)):\n",
    "\n",
    "            contour_list.append([x,y,w,h,contour[pos]])\n",
    "\n",
    "    contour_list.sort(key=lambda x : (x[1], x[0]))\n",
    "\n",
    "    tenhon2_sol = tenhon2_sol.replace(\"\\n\", \" \").replace(\" \", \",\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = os.listdir(path+\"/orignal_data\")\n",
    "\n",
    "img_name = [i for i in img_name if \"png\" in i and \"tenhon\" in i]\n",
    "\n",
    "sol_dict = dict()\n",
    "\n",
    "f = open(path+\"/orignal_data/tenhon_sol.txt\", 'r')\n",
    "\n",
    "for name, line in zip(img_name, f.readlines()):\n",
    "    line = line.replace(\"\\n\",\"\").split(\" \")\n",
    "    sol_dict[name] = line\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = path + \"/train_data/\"\n",
    "\n",
    "for file_name in sol_dict.keys():\n",
    "\n",
    "    img = cv.imread(\"./orignal_data/\" + file_name)\n",
    "    \n",
    "    mask_img = color_fliter(img, \"blue\")\n",
    "    \n",
    "    contour, hierarchy = find_contours(mask_img)\n",
    "\n",
    "    contour_list = list()\n",
    "\n",
    "    for pos in range(len(contour)):\n",
    "        \n",
    "        x, y, w, h = cv.boundingRect(contour[pos])\n",
    "\n",
    "        if ((w > 20 and w < 100 and h > 40 and h < 150) or (h > 20 and h < 100 and w > 40 and w < 150)):\n",
    "\n",
    "            contour_list.append([x,y,w,h,contour[pos]])\n",
    "\n",
    "        #     if marking:\n",
    "        #         draw_rectangle(img, x, y, w, h)\n",
    "            \n",
    "        # if showing:\n",
    "        #     show(img)\n",
    "\n",
    "    contour_list.sort(key=lambda x : (x[1], x[0]))\n",
    "\n",
    "    for (x,y,w,h,_), string in zip(contour_list, sol_dict[file_name]):\n",
    "        \n",
    "        if string in os.listdir(train_path):\n",
    "            continue\n",
    "        else:\n",
    "            pe = train_path + \"/\" + string\n",
    "            os.mkdir(pe)\n",
    "            part = img[y:y+h, x:x+w]\n",
    "\n",
    "            # cv.imwrite(pe+\"/\"+string+\".png\", part)\n",
    "\n",
    "            null_part = part * 0\n",
    "\n",
    "            count = 1\n",
    "\n",
    "            for _ in range(0,4):\n",
    "                \n",
    "                r = str(r)\n",
    "\n",
    "                for xn in range(-25, 25, 5):\n",
    "                    for yn in range(-25, 25, 5):\n",
    "\n",
    "                        w = part.shape[1]\n",
    "                        h = part.shape[0]\n",
    "                        \n",
    "                        p = part[np.where(yn > 0, yn, 0):np.where(yn > 0, h, h+yn), np.where(xn > 0, xn, 0):np.where(xn > 0, w, w+xn)]\n",
    "\n",
    "                        cv.imwrite(pe+\"/\"+string + str(count) + \".png\", p)\n",
    "\n",
    "                        count+= 1\n",
    "\n",
    "                part = cv.rotate(part, cv.ROTATE_90_CLOCKWISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = path + \"/train_data/\"\n",
    "\n",
    "if \"ztr\" not in os.listdir(train_path):\n",
    "    os.mkdir(train_path + \"/ztr\")\n",
    "\n",
    "img = cv.imread(path + \"orignal_data/dumn1.png\")\n",
    "\n",
    "mask_img = color_fliter(img, \"blue\")\n",
    "\n",
    "contour, hierarchy = find_contours(mask_img)\n",
    "\n",
    "contour_list = list()\n",
    "\n",
    "for pos in range(len(contour)):\n",
    "        \n",
    "        x, y, w, h = cv.boundingRect(contour[pos])\n",
    "\n",
    "        if ((w > 20 and w < 100 and h > 40 and h < 150) or (h > 20 and h < 100 and w > 40 and w < 150)):\n",
    "\n",
    "            contour_list.append([x,y,w,h,contour[pos]])\n",
    "\n",
    "\n",
    "for count, (x,y,w,h,_) in enumerate(contour_list):\n",
    "    \n",
    "    pe = train_path + \"/\" + \"ztr\"\n",
    "    part = img[y:y+h, x:x+w]\n",
    "    cv.imwrite(pe+\"/\"+str(count)+\".png\", part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = path + \"/train_data/\"\n",
    "\n",
    "img = cv.imread(path + \"orignal_data/dumn2.png\")\n",
    "\n",
    "mask_img = color_fliter(img, \"blue\")\n",
    "\n",
    "contour, hierarchy = find_contours(mask_img)\n",
    "\n",
    "contour_list = list()\n",
    "\n",
    "count = 0\n",
    "\n",
    "xy_list = list()\n",
    "\n",
    "for pos in range(len(contour)):\n",
    "    \n",
    "    x, y, w, h = cv.boundingRect(contour[pos])\n",
    "\n",
    "    if ((w > 20 and w < 100 and h > 40 and h < 150) or (h > 20 and h < 100 and w > 40 and w < 150)):\n",
    "\n",
    "        # contour_list.append(contour[pos])\n",
    "\n",
    "        part = img[y:y+h, x:x+w]\n",
    "\n",
    "        part = cv.resize(part, (zsize, zsize))\n",
    "\n",
    "        contour_list.append(part)\n",
    "        xy_list.append([x,y,w,h])\n",
    "\n",
    "        # if marking:\n",
    "\n",
    "        #     draw_rectangle(frame, x,y,w,h)\n",
    "\n",
    "if len(contour_list) > 0:\n",
    "\n",
    "    k = im_model.predict(np.array(contour_list), verbose=0)\n",
    "\n",
    "    k = np.argmax(k, axis=1)\n",
    "\n",
    "    for (x,y,w,h),label in zip(xy_list,k):\n",
    "\n",
    "        cv.rectangle(img, (x,y), (x+w, y+h), green, 3)\n",
    "        cv.putText(img, label_dict[label], (x, y), cv.FONT_HERSHEY_PLAIN, 2, red, 1, cv.LINE_AA)\n",
    "\n",
    "cv.imshow(\"title\", img)\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "str_dict = \"\"\"\n",
    "1s = 0\n",
    "2s = 1\n",
    "3s = 2\n",
    "4s = 3\n",
    "5s = 4\n",
    "6s = 5\n",
    "7s = 6\n",
    "8s = 7\n",
    "9s = 8\n",
    "5sr = 9\n",
    "1m = 10\n",
    "2m = 11\n",
    "3m = 12\n",
    "4m = 13\n",
    "5m = 14\n",
    "6m = 15\n",
    "7m = 16\n",
    "8m = 17\n",
    "9m = 18\n",
    "5mr = 19\n",
    "1p = 20\n",
    "2p = 21\n",
    "3p = 22\n",
    "4p = 23\n",
    "5p = 24\n",
    "6p = 25\n",
    "7p = 26\n",
    "8p = 27\n",
    "9p = 28\n",
    "5pr = 29\n",
    "ew = 30\n",
    "sw = 31\n",
    "ww = 32\n",
    "nw = 33\n",
    "wd = 34\n",
    "gd = 35\n",
    "rd = 36\n",
    "ztr = 37\n",
    "\"\"\"\n",
    "\n",
    "str_dict = \"{\" + str_dict.replace(\" \",\"\").replace(\"=\",\"\\\":\").replace(\"\\n\", \",\\\"\")[1:-2] + \"}\"\n",
    "\n",
    "str_dict = eval(str_dict)\n",
    "\n",
    "if \"train_data\" not in os.listdir(path):\n",
    "    os.mkdir(path+\"/train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = path+\"/\"+\"train_data\"\n",
    "\n",
    "file_names = os.listdir(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" dumn code \"\"\"\n",
    "\n",
    "def img_rotate(img, ro):\n",
    "    rows, cols = img.shape[0:2]\n",
    "    m45 = cv.getRotationMatrix2D((cols/2,rows/2),ro,1)\n",
    "    img45 = cv.warpAffine(img, m45, (cols,rows))\n",
    "    return img45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" dumn code \"\"\"\n",
    "\n",
    "\n",
    "for name in file_names:\n",
    "\n",
    "    if \".txt\" in name or \"ztr\" in name:\n",
    "        continue\n",
    "\n",
    "    p = cv.imread(train_path + \"/\" + name + \"/\" + name +\".png\")\n",
    "\n",
    "    for i in range(1,4):\n",
    "\n",
    "        p = cv.rotate(p, cv.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "        cv.imwrite(train_path + \"/\" + name + \"/\" + name + str(i) + \".png\", p)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, BatchNormalization, Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(zsize):\n",
    "    input_layer = Input((zsize,zsize,3))\n",
    "    conv_layer = Conv2D(16, (2,2), activation='relu')(input_layer)\n",
    "    max_layer = MaxPool2D((2,2))(conv_layer)\n",
    "    # bn_layer = BatchNormalization()(conv_layer)\n",
    "    conv_layer = Conv2D(32, (2,2), activation='relu')(max_layer)\n",
    "    max_layer = MaxPool2D((2,2))(conv_layer)\n",
    "    # bn_layer = BatchNormalization()(conv_layer)\n",
    "    conv_layer = Conv2D(64, (2,2),  activation='relu')(max_layer)\n",
    "    max_layer = MaxPool2D((2,2))(conv_layer)\n",
    "    # bn_layer = BatchNormalization()(conv_layer)\n",
    "    conv_layer = Conv2D(128, (2,2),  activation='relu')(max_layer)\n",
    "    max_layer = MaxPool2D((2,2))(conv_layer)\n",
    "    # bn_layer = BatchNormalization()(conv_layer)\n",
    "    flatten_layer = Flatten()(max_layer)\n",
    "    fc_layer = Dense(76, activation='relu')(flatten_layer)\n",
    "    fc_layer = Dense(38, activation='softmax')(fc_layer)\n",
    "\n",
    "    model = Model(input_layer, fc_layer)\n",
    "    model.compile(optimizer=Adam(1e-3), loss='sparse_categorical_crossentropy', metrics=\"accuracy\")\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 50, 50, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (None, 49, 49, 16)        208       \n",
      "                                                                 \n",
      " max_pooling2d_40 (MaxPoolin  (None, 24, 24, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_41 (Conv2D)          (None, 23, 23, 32)        2080      \n",
      "                                                                 \n",
      " max_pooling2d_41 (MaxPoolin  (None, 11, 11, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_42 (Conv2D)          (None, 10, 10, 64)        8256      \n",
      "                                                                 \n",
      " max_pooling2d_42 (MaxPoolin  (None, 5, 5, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_43 (Conv2D)          (None, 4, 4, 128)         32896     \n",
      "                                                                 \n",
      " max_pooling2d_43 (MaxPoolin  (None, 2, 2, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 76)                38988     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 38)                2926      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,354\n",
      "Trainable params: 85,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "im_model = nn_model(zsize)\n",
    "im_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = list()\n",
    "label_list = list()\n",
    "\n",
    "\n",
    "for name in file_names:\n",
    "\n",
    "    if \".txt\" in name:\n",
    "        continue\n",
    "\n",
    "    sub_list = os.listdir(train_path + \"/\" + name)\n",
    "    \n",
    "    for sub in sub_list:\n",
    "        p = cv.imread(train_path + \"/\" + name + \"/\" + sub)\n",
    "        p = cv.resize(p, (zsize,zsize))\n",
    "        image_list.append(p)\n",
    "        label_list.append(str_dict[name])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_data = list()\n",
    "noise_label = list()\n",
    "\n",
    "\n",
    "for _ in range(3000):\n",
    "\n",
    "    noise_data.append(np.random.randint(0,255,(zsize,zsize,3)))\n",
    "    noise_label.append(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "70/70 [==============================] - 10s 135ms/step - loss: 6.0697 - accuracy: 0.1846\n",
      "Epoch 2/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 2.0843 - accuracy: 0.3641\n",
      "Epoch 3/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 1.0427 - accuracy: 0.6500\n",
      "Epoch 4/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.5732 - accuracy: 0.8015\n",
      "Epoch 5/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.3927 - accuracy: 0.8592\n",
      "Epoch 6/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.3086 - accuracy: 0.8884\n",
      "Epoch 7/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.2199 - accuracy: 0.9198\n",
      "Epoch 8/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.1598 - accuracy: 0.9406\n",
      "Epoch 9/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.1317 - accuracy: 0.9485\n",
      "Epoch 10/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0983 - accuracy: 0.9602\n",
      "Epoch 11/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.1070 - accuracy: 0.9579\n",
      "Epoch 12/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0777 - accuracy: 0.9668\n",
      "Epoch 13/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0624 - accuracy: 0.9723\n",
      "Epoch 14/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0564 - accuracy: 0.9730\n",
      "Epoch 15/300\n",
      "70/70 [==============================] - 9s 131ms/step - loss: 0.0521 - accuracy: 0.9746\n",
      "Epoch 16/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0487 - accuracy: 0.9765\n",
      "Epoch 17/300\n",
      "70/70 [==============================] - 9s 132ms/step - loss: 0.0441 - accuracy: 0.9782\n",
      "Epoch 18/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0393 - accuracy: 0.9807\n",
      "Epoch 19/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0380 - accuracy: 0.9803\n",
      "Epoch 20/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0368 - accuracy: 0.9794\n",
      "Epoch 21/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0382 - accuracy: 0.9805\n",
      "Epoch 22/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0342 - accuracy: 0.9822\n",
      "Epoch 23/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0355 - accuracy: 0.9804\n",
      "Epoch 24/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0340 - accuracy: 0.9826\n",
      "Epoch 25/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.1435 - accuracy: 0.9488\n",
      "Epoch 26/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0924 - accuracy: 0.9615\n",
      "Epoch 27/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0449 - accuracy: 0.9780\n",
      "Epoch 28/300\n",
      "70/70 [==============================] - 9s 131ms/step - loss: 0.0348 - accuracy: 0.9818\n",
      "Epoch 29/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0321 - accuracy: 0.9827\n",
      "Epoch 30/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0301 - accuracy: 0.9846\n",
      "Epoch 31/300\n",
      "70/70 [==============================] - 9s 131ms/step - loss: 0.0284 - accuracy: 0.9857\n",
      "Epoch 32/300\n",
      "70/70 [==============================] - 10s 136ms/step - loss: 0.0288 - accuracy: 0.9851\n",
      "Epoch 33/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0295 - accuracy: 0.9860\n",
      "Epoch 34/300\n",
      "70/70 [==============================] - 9s 131ms/step - loss: 0.0276 - accuracy: 0.9857\n",
      "Epoch 35/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0254 - accuracy: 0.9870\n",
      "Epoch 36/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0264 - accuracy: 0.9871\n",
      "Epoch 37/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0215 - accuracy: 0.9911\n",
      "Epoch 38/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0202 - accuracy: 0.9916\n",
      "Epoch 39/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0499 - accuracy: 0.9807\n",
      "Epoch 40/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.1650 - accuracy: 0.9433\n",
      "Epoch 41/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0304 - accuracy: 0.9867\n",
      "Epoch 42/300\n",
      "70/70 [==============================] - 9s 131ms/step - loss: 0.0228 - accuracy: 0.9909\n",
      "Epoch 43/300\n",
      "70/70 [==============================] - 9s 132ms/step - loss: 0.0223 - accuracy: 0.9912\n",
      "Epoch 44/300\n",
      "70/70 [==============================] - 9s 131ms/step - loss: 0.0249 - accuracy: 0.9888\n",
      "Epoch 45/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0256 - accuracy: 0.9889\n",
      "Epoch 46/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0156 - accuracy: 0.9938\n",
      "Epoch 47/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0158 - accuracy: 0.9930\n",
      "Epoch 48/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0111 - accuracy: 0.9962\n",
      "Epoch 49/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0087 - accuracy: 0.9975\n",
      "Epoch 50/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0103 - accuracy: 0.9966\n",
      "Epoch 51/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0106 - accuracy: 0.9958\n",
      "Epoch 52/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0147 - accuracy: 0.9941\n",
      "Epoch 53/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0048 - accuracy: 0.9992\n",
      "Epoch 54/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0034 - accuracy: 0.9995\n",
      "Epoch 55/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0028 - accuracy: 0.9997\n",
      "Epoch 56/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0015 - accuracy: 0.9999\n",
      "Epoch 57/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 58/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 9.0653e-04 - accuracy: 1.0000\n",
      "Epoch 59/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 7.7071e-04 - accuracy: 1.0000\n",
      "Epoch 60/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 7.8902e-04 - accuracy: 1.0000\n",
      "Epoch 61/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 6.2834e-04 - accuracy: 1.0000\n",
      "Epoch 62/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 5.4892e-04 - accuracy: 1.0000\n",
      "Epoch 63/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 64/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.1289 - accuracy: 0.9608\n",
      "Epoch 65/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0758 - accuracy: 0.9713\n",
      "Epoch 66/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 0.0271 - accuracy: 0.9901\n",
      "Epoch 67/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0652 - accuracy: 0.9773\n",
      "Epoch 68/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0414 - accuracy: 0.9837\n",
      "Epoch 69/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0090 - accuracy: 0.9975\n",
      "Epoch 70/300\n",
      "70/70 [==============================] - 9s 132ms/step - loss: 0.0041 - accuracy: 0.9992\n",
      "Epoch 71/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0022 - accuracy: 0.9999\n",
      "Epoch 72/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 73/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 7.6231e-04 - accuracy: 1.0000\n",
      "Epoch 74/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 5.8763e-04 - accuracy: 1.0000\n",
      "Epoch 75/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 5.0912e-04 - accuracy: 1.0000\n",
      "Epoch 76/300\n",
      "70/70 [==============================] - 9s 132ms/step - loss: 4.3690e-04 - accuracy: 1.0000\n",
      "Epoch 77/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 3.6726e-04 - accuracy: 1.0000\n",
      "Epoch 78/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 3.2845e-04 - accuracy: 1.0000\n",
      "Epoch 79/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 2.8155e-04 - accuracy: 1.0000\n",
      "Epoch 80/300\n",
      "70/70 [==============================] - 9s 129ms/step - loss: 2.5393e-04 - accuracy: 1.0000\n",
      "Epoch 81/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 2.3373e-04 - accuracy: 1.0000\n",
      "Epoch 82/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 2.0752e-04 - accuracy: 1.0000\n",
      "Epoch 83/300\n",
      "70/70 [==============================] - 9s 130ms/step - loss: 1.8537e-04 - accuracy: 1.0000\n",
      "Epoch 84/300\n",
      "51/70 [====================>.........] - ETA: 2s - loss: 1.6962e-04 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\opencv_test\\test.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m im_model \u001b[39m=\u001b[39m nn_model(zsize)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m im_model\u001b[39m.\u001b[39;49mfit(np\u001b[39m.\u001b[39;49marray(image_list \u001b[39m+\u001b[39;49m noise_data), np\u001b[39m.\u001b[39;49marray(label_list \u001b[39m+\u001b[39;49m noise_label), batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "im_model = nn_model(zsize)\n",
    "im_model.fit(np.array(image_list + noise_data), np.array(label_list + noise_label), batch_size=256, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = dict((v,k) for k, v in str_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(path+video_name)\n",
    "\n",
    "while (cap.isOpened()):\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    mask_img = color_fliter(frame, \"blue\")\n",
    "    \n",
    "    contour, hierarchy = find_contours(mask_img)\n",
    "\n",
    "    contour_list = list()\n",
    "\n",
    "    xy_list = list()\n",
    "    \n",
    "    for pos in range(len(contour)):\n",
    "        \n",
    "        x, y, w, h = cv.boundingRect(contour[pos])\n",
    "\n",
    "        if ((w > 20 and w < 100 and h > 40 and h < 150) or (h > 20 and h < 100 and w > 40 and w < 150)):\n",
    "\n",
    "            # contour_list.append(contour[pos])\n",
    "\n",
    "            part = frame[y+int(h*0.05):y+int(h*0.95), x+int(w*0.05):x+int(w*0.95)]\n",
    "\n",
    "            part = cv.resize(part, (zsize, zsize))\n",
    "\n",
    "            contour_list.append(part)\n",
    "            xy_list.append([x,y,w,h])\n",
    "\n",
    "            # if marking:\n",
    "\n",
    "            #     draw_rectangle(frame, x,y,w,h)\n",
    "\n",
    "    if len(contour_list) > 0:\n",
    "\n",
    "        k = im_model.predict(np.array(contour_list), verbose=0)\n",
    "\n",
    "        k = np.argmax(k, axis=1)\n",
    "\n",
    "        for (x,y,w,h),label in zip(xy_list,k):\n",
    "\n",
    "            if label < 37:\n",
    "\n",
    "                cv.rectangle(frame, (x,y), (x+w, y+h), green, 3)\n",
    "                cv.putText(frame, label_dict[label], (x, y), cv.FONT_HERSHEY_PLAIN, 2, red, 1, cv.LINE_AA)\n",
    "\n",
    "    isBreak = show(frame)\n",
    "        \n",
    "    if isBreak:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "\n",
    "#read image\n",
    "img = cv2.imread(\"./train_data/1m/1m.png\")\n",
    "\n",
    "# change size of image\n",
    "width = int(img.shape[1])\n",
    "height = int(img.shape[0])\n",
    "dim = (width, height)\n",
    "\n",
    "min_color = int(np.max(img))\n",
    "min_color = (min_color, min_color, min_color)\n",
    "\n",
    "# img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "#2d to 3d (projection)  , and -> rotation point - center point (origin point)\n",
    "proj2dto3d = np.array([[1,0,-img.shape[1]],\n",
    "                      [0,1,-img.shape[0]],\n",
    "                      [0,0,0],\n",
    "                      [0,0,1]],np.float32)\n",
    "\n",
    "# 3d matrixs in  x ,y ,z \n",
    "\n",
    "'''\n",
    " you can remove any matrix if you dont want to rotate around it , so in our case \n",
    " we rotate around y axis only so any line ends with \" #0 \" we can remove it \n",
    " and the programe will run \n",
    "\n",
    "'''\n",
    "rx   = np.array([[1,0,0,0],\n",
    "                 [0,1,0,0],\n",
    "                 [0,0,1,0],\n",
    "                 [0,0,0,1]],np.float32)\n",
    "\n",
    "ry   = np.array([[1,0,0,0],\n",
    "                 [0,1,0,0],\n",
    "                 [0,0,1,0],\n",
    "                 [0,0,0,1]],np.float32)\n",
    "\n",
    "rz   = np.array([[1,0,0,0],\n",
    "                 [0,1,0,0],\n",
    "                 [0,0,1,0],\n",
    "                 [0,0,0,1]],np.float32)\n",
    "\n",
    "\n",
    "trans= np.array([[1,0,0,0],\n",
    "                 [0,1,0,0],\n",
    "                 [0,0,1,height+20],\n",
    "                 [0,0,0,1]],np.float32)\n",
    "\n",
    "\n",
    "\n",
    "proj3dto2d = np.array([ [width,0,img.shape[1],0],\n",
    "                        [0,height,img.shape[0],0],\n",
    "                        [0,0,1,0] ],np.float32)\n",
    "\n",
    "x = -40.0 #0\n",
    "y = 0.0\n",
    "z = 0.0 #0\n",
    "\n",
    "\n",
    "ax = float(x * (math.pi / 180.0)) #0\n",
    "ay = float(y * (math.pi / 180.0)) \n",
    "az = float(z * (math.pi / 180.0)) #0\n",
    "\n",
    "rx[1,1] = math.cos(ax) #0\n",
    "rx[1,2] = -math.sin(ax) #0\n",
    "rx[2,1] = math.sin(ax) #0\n",
    "rx[2,2] = math.cos(ax) #0\n",
    "\n",
    "ry[0,0] = math.cos(ay)\n",
    "ry[0,2] = -math.sin(ay)\n",
    "ry[2,0] = math.sin(ay)\n",
    "ry[2,2] = math.cos(ay)\n",
    "\n",
    "rz[0,0] = math.cos(az) #0\n",
    "rz[0,1] = -math.sin(az) #0\n",
    "rz[1,0] = math.sin(az) #0\n",
    "rz[1,1] = math.cos(az) #0\n",
    "\n",
    "r =rx.dot(ry).dot(rz) # if we remove the lines we put    r=ry\n",
    "final = proj3dto2d.dot(trans.dot(r.dot(proj2dto3d)))\n",
    "\n",
    "dst = cv2.warpPerspective(img, final,(img.shape[1],img.shape[0]),None,cv2.INTER_LINEAR\n",
    "                            ,cv2.BORDER_CONSTANT,min_color)\n",
    "\n",
    "\n",
    "cv2.imshow(\"dst\",dst)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rotate_img(img):\n",
    "\n",
    "    width = int(img.shape[1])\n",
    "    height = int(img.shape[0])\n",
    "    \n",
    "    rx   = np.array([[1,0,0,0],\n",
    "                    [0,1,0,0],\n",
    "                    [0,0,1,0],\n",
    "                    [0,0,0,1]],np.float32)\n",
    "\n",
    "    ry   = np.array([[1,0,0,0],\n",
    "                    [0,1,0,0],\n",
    "                    [0,0,1,0],\n",
    "                    [0,0,0,1]],np.float32)\n",
    "\n",
    "    rz   = np.array([[1,0,0,0],\n",
    "                    [0,1,0,0],\n",
    "                    [0,0,1,0],\n",
    "                    [0,0,0,1]],np.float32)\n",
    "\n",
    "\n",
    "    trans= np.array([[1,0,0,0],\n",
    "                    [0,1,0,0],\n",
    "                    [0,0,1,height+30],\n",
    "                    [0,0,0,1]],np.float32)\n",
    "\n",
    "\n",
    "    proj3dto2d = np.array([ [100,0,height/2,0],\n",
    "                            [0,100,width/2,0],\n",
    "                            [0,0,1,0] ],np.float32)\n",
    "\n",
    "    dim = (width, height)\n",
    "\n",
    "    min_color = (int(np.max(img)), int(np.max(img)), int(np.max(img)))\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53c7ce644e3f6210fe490b2e7844b26f537b2740ab8b6d88e5c764ce633e4d3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
