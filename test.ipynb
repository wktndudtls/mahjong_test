{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import pyautogui\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "video_name = 'test_video.mp4'\n",
    "tenhon_name = 'tenhon.png'\n",
    "tenhon_name2 = 'tenhon2.png'\n",
    "global rc\n",
    "rc = 200\n",
    "global red\n",
    "red = (0,0,255)\n",
    "global green\n",
    "green = (0,255,0)\n",
    "global blue\n",
    "blue = (255,0,0)\n",
    "global marking\n",
    "marking = True\n",
    "showing = True\n",
    "zsize = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_range(str):\n",
    "    \n",
    "    if str == \"blue\":\n",
    "        l = np.array([rc,0,0])\n",
    "        \n",
    "    elif str == \"green\":\n",
    "        l = np.array([0, rc, 0])\n",
    "    elif str == \"red\":\n",
    "        l = np.array([0, 0, rc])\n",
    "\n",
    "    u = np.array([255,255,255])\n",
    "\n",
    "    return l, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img):\n",
    "    cv.imshow(\"title\", img)\n",
    "        \n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        cv.destroyAllWindows()\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    cv.imshow(\"title\", img)\n",
    "    cv.waitKey()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwhile True:\\n\\n    screenshot = pyautogui.screenshot()\\n    screenshot = np.array(screenshot)\\n    screenshot = cv.cvtColor(screenshot, cv.COLOR_RGB2BGR)\\n    cv.imshow('', screenshot)\\n\\n    if cv.waitKey(1) == ord('q'):\\n        cv.destroyAllWindows()\\n        break\\n\\nprint('Done')\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "while True:\n",
    "\n",
    "    screenshot = pyautogui.screenshot()\n",
    "    screenshot = np.array(screenshot)\n",
    "    screenshot = cv.cvtColor(screenshot, cv.COLOR_RGB2BGR)\n",
    "    cv.imshow('', screenshot)\n",
    "\n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        cv.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "print('Done')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_fliter(img, string):\n",
    "    l, u = color_range(string)\n",
    "    mask = cv.inRange(img, l, u)\n",
    "    img = cv.bitwise_or(img, img, mask=mask)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contours(img):\n",
    "\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    im_thresh = cv.adaptiveThreshold(gray, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 21, 0)\n",
    "\n",
    "    return cv.findContours(im_thresh, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rectangle(img, x, y, w, h):\n",
    "    cv.rectangle(img, (x,y), (x+w, y+h), green, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text(img, x, y, string):\n",
    "    cv.putText(img, string, (x, y), cv.FONT_HERSHEY_PLAIN, 1, red, 1, cv.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(path+video_name)\n",
    "\n",
    "while (cap.isOpened()):\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    mask_img = color_fliter(frame, \"blue\")\n",
    "    \n",
    "    contour, hierarchy = find_contours(mask_img)\n",
    "\n",
    "    contour_list = list()\n",
    "    \n",
    "    for pos in range(len(contour)):\n",
    "        \n",
    "        x, y, w, h = cv.boundingRect(contour[pos])\n",
    "\n",
    "        if ((w > 20 and w < 100 and h > 40 and h < 150) or (h > 20 and h < 100 and w > 40 and w < 150)):\n",
    "\n",
    "            contour_list.append(contour[pos])\n",
    "\n",
    "            if marking:\n",
    "\n",
    "                draw_rectangle(frame)\n",
    "        \n",
    "    isBreak = show(frame)\n",
    "        \n",
    "    if isBreak:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'cv::inRange'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32me:\\opencv_test\\test.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m [tenhon_name, tenhon_name2]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     solimg \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39mimread(path \u001b[39m+\u001b[39m tenhon_name2)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     mask_img \u001b[39m=\u001b[39m color_fliter(solimg, \u001b[39m\"\u001b[39;49m\u001b[39mblue\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     contour, hierarchy \u001b[39m=\u001b[39m find_contours(mask_img)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     contour_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n",
      "\u001b[1;32me:\\opencv_test\\test.ipynb Cell 11\u001b[0m in \u001b[0;36mcolor_fliter\u001b[1;34m(img, string)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcolor_fliter\u001b[39m(img, string):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     l, u \u001b[39m=\u001b[39m color_range(string)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     mask \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39;49minRange(img, l, u)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     img \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39mbitwise_or(img, img, mask\u001b[39m=\u001b[39mmask)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\arithm.cpp:1726: error: (-215:Assertion failed) ! _src.empty() in function 'cv::inRange'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name in [tenhon_name, tenhon_name2]:\n",
    "    \n",
    "    solimg = cv.imread(path + tenhon_name2)\n",
    "\n",
    "    mask_img = color_fliter(solimg, \"blue\")\n",
    "\n",
    "    contour, hierarchy = find_contours(mask_img)\n",
    "\n",
    "    contour_list = list()\n",
    "\n",
    "    for pos in range(len(contour)):\n",
    "        \n",
    "        x, y, w, h = cv.boundingRect(contour[pos])\n",
    "\n",
    "        if ((w > 20 and w < 100 and h > 40 and h < 150) or (h > 20 and h < 100 and w > 40 and w < 150)):\n",
    "\n",
    "            contour_list.append([x,y,w,h,contour[pos]])\n",
    "\n",
    "    contour_list.sort(key=lambda x : (x[1], x[0]))\n",
    "\n",
    "    tenhon2_sol = tenhon2_sol.replace(\"\\n\", \" \").replace(\" \", \",\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = os.listdir(path+\"/orignal_data\")\n",
    "\n",
    "img_name = [i for i in img_name if \"png\" in i and \"tenhon\" in i]\n",
    "\n",
    "sol_dict = dict()\n",
    "\n",
    "f = open(path+\"/orignal_data/tenhon_sol.txt\", 'r')\n",
    "\n",
    "for name, line in zip(img_name, f.readlines()):\n",
    "    line = line.replace(\"\\n\",\"\").split(\" \")\n",
    "    sol_dict[name] = line\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = path + \"/train_data/\"\n",
    "\n",
    "for file_name in sol_dict.keys():\n",
    "\n",
    "    img = cv.imread(\"./orignal_data/\" + file_name)\n",
    "    \n",
    "    mask_img = color_fliter(img, \"blue\")\n",
    "    \n",
    "    contour, hierarchy = find_contours(mask_img)\n",
    "\n",
    "    contour_list = list()\n",
    "\n",
    "    for pos in range(len(contour)):\n",
    "        \n",
    "        x, y, w, h = cv.boundingRect(contour[pos])\n",
    "\n",
    "        if ((w > 20 and w < 100 and h > 40 and h < 150) or (h > 20 and h < 100 and w > 40 and w < 150)):\n",
    "\n",
    "            contour_list.append([x,y,w,h,contour[pos]])\n",
    "\n",
    "        #     if marking:\n",
    "        #         draw_rectangle(img, x, y, w, h)\n",
    "            \n",
    "        # if showing:\n",
    "        #     show(img)\n",
    "\n",
    "    contour_list.sort(key=lambda x : (x[1], x[0]))\n",
    "\n",
    "    # img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    for (x,y,w,h,_), string in zip(contour_list, sol_dict[file_name]):\n",
    "        \n",
    "        if string in os.listdir(train_path):\n",
    "            continue\n",
    "        else:\n",
    "            pe = train_path + \"/\" + string\n",
    "            os.mkdir(pe)\n",
    "\n",
    "            part = img[y:y+h, x:x+w]\n",
    "\n",
    "            # part = edge_white(part, w, h)\n",
    "\n",
    "            # part = cv.Canny(part, 255,255)\n",
    "\n",
    "            cv.imwrite(pe+\"/\"+string + \"ori\" + \".png\", part)\n",
    "\n",
    "            count = 1\n",
    "\n",
    "            for _ in range(0,4):\n",
    "                \n",
    "                for xn in range(-4, 4, 2):\n",
    "                    for yn in range(-6, 6, 2):\n",
    "\n",
    "                        if len(string) > 2:\n",
    "\n",
    "                            if string[2] == \"r\" and (abs(xn) > 4 or abs(yn) > 6):\n",
    "                                continue\n",
    "\n",
    "                        if x > y:\n",
    "                            temp = yn\n",
    "                            yn = xn\n",
    "                            xn = temp\n",
    "\n",
    "                        w = part.shape[1]\n",
    "                        h = part.shape[0]\n",
    "                        \n",
    "                        p = part[np.where(yn > 0, yn, 0):np.where(yn > 0, h, h+yn), np.where(xn > 0, xn, 0):np.where(xn > 0, w, w+xn)]\n",
    "\n",
    "                        for size in range(0,50,10):\n",
    "                            ps = cv.resize(p, (int(p.shape[1] * ((100-size)/100)), int(p.shape[0] * ((100-size)/100))))\n",
    "\n",
    "                            cv.imwrite(pe+\"/\"+string + str(count) + \".png\", ps)\n",
    "\n",
    "                            count+= 1\n",
    "\n",
    "                part = cv.rotate(part, cv.ROTATE_90_CLOCKWISE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = path + \"/train_data/\"\n",
    "\n",
    "if \"ztr\" not in os.listdir(train_path):\n",
    "    os.mkdir(train_path + \"/ztr\")\n",
    "\n",
    "img = cv.imread(path + \"orignal_data/dumn1.png\")\n",
    "\n",
    "mask_img = color_fliter(img, \"blue\")\n",
    "\n",
    "contour, hierarchy = find_contours(mask_img)\n",
    "\n",
    "contour_list = list()\n",
    "\n",
    "for pos in range(len(contour)):\n",
    "        \n",
    "        x, y, w, h = cv.boundingRect(contour[pos])\n",
    "\n",
    "        if ((w > 20 and w < 100 and h > 40 and h < 150) or (h > 20 and h < 100 and w > 40 and w < 150)):\n",
    "\n",
    "            contour_list.append([x,y,w,h,contour[pos]])\n",
    "\n",
    "\n",
    "for count, (x,y,w,h,_) in enumerate(contour_list):\n",
    "    \n",
    "    pe = train_path + \"/\" + \"ztr\"\n",
    "    part = img[y:y+h, x:x+w]\n",
    "    cv.imwrite(pe+\"/\"+str(count)+\".png\", part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = path + \"/train_data/\"\n",
    "\n",
    "img = cv.imread(path + \"orignal_data/dumn2.png\")\n",
    "\n",
    "mask_img = color_fliter(img, \"blue\")\n",
    "\n",
    "contour, hierarchy = find_contours(mask_img)\n",
    "\n",
    "contour_list = list()\n",
    "\n",
    "count = 0\n",
    "\n",
    "xy_list = list()\n",
    "\n",
    "for pos in range(len(contour)):\n",
    "    \n",
    "    x, y, w, h = cv.boundingRect(contour[pos])\n",
    "\n",
    "    if ((w > 20 and w < 100 and h > 40 and h < 150) or (h > 20 and h < 100 and w > 40 and w < 150)):\n",
    "\n",
    "        # contour_list.append(contour[pos])\n",
    "\n",
    "        part = img[y+int(h*0.1):y+int(h*0.9), x+int(w*0.1):x+int(w*0.9)]\n",
    "\n",
    "        part = cv.resize(part, (zsize, zsize))\n",
    "\n",
    "        contour_list.append(part)\n",
    "        xy_list.append([x,y,w,h])\n",
    "\n",
    "        # if marking:\n",
    "\n",
    "        #     draw_rectangle(frame, x,y,w,h)\n",
    "\n",
    "if len(contour_list) > 0:\n",
    "\n",
    "    k = im_model.predict(np.array(contour_list), verbose=0)\n",
    "\n",
    "    k = np.argmax(k, axis=1)\n",
    "\n",
    "    for (x,y,w,h),label in zip(xy_list,k):\n",
    "\n",
    "        cv.rectangle(img, (x,y), (x+w, y+h), green, 3)\n",
    "        cv.putText(img, label_dict[label], (x, y), cv.FONT_HERSHEY_PLAIN, 2, red, 1, cv.LINE_AA)\n",
    "\n",
    "cv.imshow(\"title\", img)\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "str_dict = \"\"\"\n",
    "1s = 0\n",
    "2s = 1\n",
    "3s = 2\n",
    "4s = 3\n",
    "5s = 4\n",
    "6s = 5\n",
    "7s = 6\n",
    "8s = 7\n",
    "9s = 8\n",
    "5sr = 9\n",
    "1m = 10\n",
    "2m = 11\n",
    "3m = 12\n",
    "4m = 13\n",
    "5m = 14\n",
    "6m = 15\n",
    "7m = 16\n",
    "8m = 17\n",
    "9m = 18\n",
    "5mr = 19\n",
    "1p = 20\n",
    "2p = 21\n",
    "3p = 22\n",
    "4p = 23\n",
    "5p = 24\n",
    "6p = 25\n",
    "7p = 26\n",
    "8p = 27\n",
    "9p = 28\n",
    "5pr = 29\n",
    "ew = 30\n",
    "sw = 31\n",
    "ww = 32\n",
    "nw = 33\n",
    "wd = 34\n",
    "gd = 35\n",
    "rd = 36\n",
    "ztr = 37\n",
    "\"\"\"\n",
    "\n",
    "str_dict = \"{\" + str_dict.replace(\" \",\"\").replace(\"=\",\"\\\":\").replace(\"\\n\", \",\\\"\")[1:-2] + \"}\"\n",
    "\n",
    "str_dict = eval(str_dict)\n",
    "\n",
    "if \"train_data\" not in os.listdir(path):\n",
    "    os.mkdir(path+\"/train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = path+\"/\"+\"train_data\"\n",
    "\n",
    "file_names = os.listdir(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" dumn code \"\"\"\n",
    "\n",
    "def img_rotate(img, ro):\n",
    "    rows, cols = img.shape[0:2]\n",
    "    m45 = cv.getRotationMatrix2D((cols/2,rows/2),ro,1)\n",
    "    img45 = cv.warpAffine(img, m45, (cols,rows))\n",
    "    return img45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:801: error: (-215:Assertion failed) !_img.empty() in function 'cv::imwrite'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32me:\\opencv_test\\test.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     p \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39mrotate(p, cv\u001b[39m.\u001b[39mROTATE_90_CLOCKWISE)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     cv\u001b[39m.\u001b[39;49mimwrite(train_path \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(i) \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.png\u001b[39;49m\u001b[39m\"\u001b[39;49m, p)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:801: error: (-215:Assertion failed) !_img.empty() in function 'cv::imwrite'\n"
     ]
    }
   ],
   "source": [
    "\"\"\" dumn code \"\"\"\n",
    "\n",
    "\n",
    "for name in file_names:\n",
    "\n",
    "    if \".txt\" in name or \"ztr\" in name:\n",
    "        continue\n",
    "\n",
    "    p = cv.imread(train_path + \"/\" + name + \"/\" + name +\".png\")\n",
    "\n",
    "    for i in range(1,4):\n",
    "\n",
    "        p = cv.rotate(p, cv.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "        cv.imwrite(train_path + \"/\" + name + \"/\" + name + str(i) + \".png\", p)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, BatchNormalization, Input, Flatten, Conv1D, MaxPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(zsize):\n",
    "    input_layer = Input((zsize,zsize,3))\n",
    "    conv_layer = Conv2D(16, (2,2), activation='relu')(input_layer)\n",
    "    max_layer = MaxPool2D((2,2))(conv_layer)\n",
    "    # bn_layer = BatchNormalization()(max_layer)\n",
    "    conv_layer = Conv2D(32, (2,2), activation='relu')(max_layer)\n",
    "    max_layer = MaxPool2D((2,2))(conv_layer)\n",
    "    # bn_layer = BatchNormalization()(max_layer)\n",
    "    conv_layer = Conv2D(64, (2,2),  activation='relu')(max_layer)\n",
    "    max_layer = MaxPool2D((2,2))(conv_layer)\n",
    "    bn_layer = BatchNormalization()(max_layer)\n",
    "    conv_layer = Conv2D(128, (2,2),  activation='relu')(bn_layer)\n",
    "    max_layer = MaxPool2D((2,2))(conv_layer)\n",
    "    # bn_layer = BatchNormalization()(max_layer)\n",
    "    # conv_layer = Conv2D(128, (2,2),  activation='relu')(bn_layer)\n",
    "    # max_layer = MaxPool2D((2,2))(conv_layer)\n",
    "    flatten_layer = Flatten()(max_layer)\n",
    "    fc_layer = Dense(76, activation='relu')(flatten_layer)\n",
    "    fc_layer = Dense(38, activation='softmax')(fc_layer)\n",
    "\n",
    "    model = Model(input_layer, fc_layer)\n",
    "    model.compile(optimizer=Adam(1e-3), loss='sparse_categorical_crossentropy', metrics=\"accuracy\")\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray_nn_model(zsize):\n",
    "    input_layer = Input((zsize,zsize))\n",
    "    conv_layer = Conv1D(16, 2, activation='relu')(input_layer)\n",
    "    max_layer = MaxPool1D(2)(conv_layer)\n",
    "    bn_layer = BatchNormalization()(max_layer)\n",
    "    conv_layer = Conv1D(32, 2, activation='relu')(bn_layer)\n",
    "    max_layer = MaxPool1D(2)(conv_layer)\n",
    "    bn_layer = BatchNormalization()(max_layer)\n",
    "    conv_layer = Conv1D(64, 2, activation='relu')(bn_layer)\n",
    "    max_layer = MaxPool1D(2)(conv_layer)\n",
    "    bn_layer = BatchNormalization()(max_layer)\n",
    "    conv_layer = Conv1D(64, 2, activation='relu')(bn_layer)\n",
    "    max_layer = MaxPool1D(2)(conv_layer)\n",
    "    # bn_layer = BatchNormalization()(max_layer)\n",
    "    flatten_layer = Flatten()(max_layer)\n",
    "    fc_layer = Dense(38, activation='relu')(flatten_layer)\n",
    "    # bn_layer = BatchNormalization()(fc_layer)\n",
    "    fc_layer = Dense(38, activation='softmax')(fc_layer)\n",
    "    \n",
    "    model = Model(input_layer, fc_layer)\n",
    "    model.compile(optimizer=Adam(1e-3), loss='sparse_categorical_crossentropy', metrics=\"accuracy\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 90, 90, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 89, 89, 16)        208       \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 44, 44, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 43, 43, 32)        2080      \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, 21, 21, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 20, 20, 64)        8256      \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPoolin  (None, 10, 10, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 10, 10, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 9, 9, 128)         32896     \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPoolin  (None, 4, 4, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 76)                155724    \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 38)                2926      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 202,346\n",
      "Trainable params: 202,218\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "im_model = nn_model(zsize)\n",
    "im_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = list()\n",
    "label_list = list()\n",
    "\n",
    "\n",
    "for name in file_names:\n",
    "\n",
    "    if \".txt\" in name:\n",
    "        continue\n",
    "\n",
    "    sub_list = os.listdir(train_path + \"/\" + name)\n",
    "    \n",
    "    for sub in sub_list:\n",
    "        p = cv.imread(train_path + \"/\" + name + \"/\" + sub)\n",
    "        p = cv.resize(p, (zsize,zsize))\n",
    "\n",
    "        image_list.append(p)\n",
    "        label_list.append(str_dict[name])\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_data = list()\n",
    "noise_label = list()\n",
    "\n",
    "\n",
    "for _ in range(77500):\n",
    "\n",
    "    n = np.random.randint(0,255,(zsize,zsize,3))\n",
    "    # n = np.where(n > 125, 255, 0)\n",
    "\n",
    "    noise_data.append(n)\n",
    "    noise_label.append(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "93/93 [==============================] - 171s 2s/step - loss: 0.4648 - accuracy: 0.8796\n",
      "Epoch 2/300\n",
      "93/93 [==============================] - 157s 2s/step - loss: 0.0291 - accuracy: 0.9950\n",
      "Epoch 3/300\n",
      "93/93 [==============================] - 153s 2s/step - loss: 0.0032 - accuracy: 0.9999\n",
      "Epoch 4/300\n",
      "93/93 [==============================] - 154s 2s/step - loss: 9.8859e-04 - accuracy: 1.0000\n",
      "Epoch 5/300\n",
      "93/93 [==============================] - 153s 2s/step - loss: 5.1972e-04 - accuracy: 1.0000\n",
      "Epoch 6/300\n",
      "93/93 [==============================] - 155s 2s/step - loss: 3.3884e-04 - accuracy: 1.0000\n",
      "Epoch 7/300\n",
      "93/93 [==============================] - 153s 2s/step - loss: 2.3833e-04 - accuracy: 1.0000\n",
      "Epoch 8/300\n",
      "93/93 [==============================] - 153s 2s/step - loss: 1.7317e-04 - accuracy: 1.0000\n",
      "Epoch 9/300\n",
      "93/93 [==============================] - 153s 2s/step - loss: 1.3697e-04 - accuracy: 1.0000\n",
      "Epoch 10/300\n",
      " 2/93 [..............................] - ETA: 2:39 - loss: 1.7200e-04 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\opencv_test\\test.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# im_model.compile(optimizer=Adam(1e-5), loss='sparse_categorical_crossentropy', metrics=\"accuracy\")\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39m/GPU:1\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/opencv_test/test.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     im_model\u001b[39m.\u001b[39;49mfit(np\u001b[39m.\u001b[39;49marray(image_list \u001b[39m+\u001b[39;49m noise_data), np\u001b[39m.\u001b[39;49marray(label_list \u001b[39m+\u001b[39;49m noise_label), batch_size\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\sangmin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "im_model = nn_model(zsize)\n",
    "\n",
    "# im_model.compile(optimizer=Adam(1e-5), loss='sparse_categorical_crossentropy', metrics=\"accuracy\")\n",
    "\n",
    "with tf.device('/GPU:1'):\n",
    "\n",
    "    im_model.fit(np.array(image_list + noise_data), np.array(label_list + noise_label), batch_size=1024, epochs=300)\n",
    "    # im_model = nn_model(zsize)\n",
    "    # im_model.fit(np.array(image_list), np.array(label_list), batch_size=256, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = dict((v,k) for k, v in str_dict.items())\n",
    "\n",
    "cap = cv.VideoCapture(path+video_name)\n",
    "\n",
    "while (cap.isOpened()):\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    mask_img = color_fliter(frame, \"blue\")\n",
    "    \n",
    "    contour, hierarchy = find_contours(mask_img)\n",
    "\n",
    "    contour_list = list()\n",
    "\n",
    "    xy_list = list()\n",
    "\n",
    "    # img_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # img_gray = cv.Canny(img_gray, 220, 220)\n",
    "\n",
    "    # img_gray = np.where(img_gray > 200 , 255, img_gray)\n",
    "    \n",
    "    for pos in range(len(contour)):\n",
    "        \n",
    "        x, y, w, h = cv.boundingRect(contour[pos])\n",
    "\n",
    "        if ((w > 20 and w < 100 and h > 40 and h < 150) or (h > 20 and h < 100 and w > 40 and w < 150)):\n",
    "\n",
    "            # contour_list.append(contour[pos])\n",
    "\n",
    "            # part = img_gray[y+int(h*0.1):y+int(h*0.9), x+int(w*0.1):x+int(w*0.9)]\n",
    "            part = frame[y:y+h, x:x+w]\n",
    "\n",
    "            # part = cv.Canny(part, 255,255)\n",
    "\n",
    "            part = cv.resize(part, (zsize, zsize))\n",
    "\n",
    "            contour_list.append(part)\n",
    "            xy_list.append([x,y,w,h])\n",
    "\n",
    "            # if marking:\n",
    "\n",
    "            #     draw_rectangle(frame, x,y,w,h)\n",
    "\n",
    "    if len(contour_list) > 0:\n",
    "\n",
    "        k = im_model.predict(np.array(contour_list), verbose=0)\n",
    "\n",
    "        k = np.argmax(k, axis=1)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for (x,y,w,h),label in zip(xy_list,k):\n",
    "\n",
    "            if label < 37:\n",
    "\n",
    "                cv.rectangle(frame, (x,y), (x+w, y+h), green, 3)\n",
    "                cv.putText(frame, label_dict[label], (x, y), cv.FONT_HERSHEY_PLAIN, 2, red, 1, cv.LINE_AA)\n",
    "\n",
    "    isBreak = show(frame)\n",
    "        \n",
    "    if isBreak:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "\n",
    "#read image\n",
    "img = cv2.imread(\"./train_data/1m/1m.png\")\n",
    "\n",
    "# change size of image\n",
    "width = int(img.shape[1])\n",
    "height = int(img.shape[0])\n",
    "dim = (width, height)\n",
    "\n",
    "min_color = int(np.max(img))\n",
    "min_color = (min_color, min_color, min_color)\n",
    "\n",
    "# img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "#2d to 3d (projection)  , and -> rotation point - center point (origin point)\n",
    "proj2dto3d = np.array([[1,0,-img.shape[1]],\n",
    "                      [0,1,-img.shape[0]],\n",
    "                      [0,0,0],\n",
    "                      [0,0,1]],np.float32)\n",
    "\n",
    "# 3d matrixs in  x ,y ,z \n",
    "\n",
    "'''\n",
    " you can remove any matrix if you dont want to rotate around it , so in our case \n",
    " we rotate around y axis only so any line ends with \" #0 \" we can remove it \n",
    " and the programe will run \n",
    "\n",
    "'''\n",
    "rx   = np.array([[1,0,0,0],\n",
    "                 [0,1,0,0],\n",
    "                 [0,0,1,0],\n",
    "                 [0,0,0,1]],np.float32)\n",
    "\n",
    "ry   = np.array([[1,0,0,0],\n",
    "                 [0,1,0,0],\n",
    "                 [0,0,1,0],\n",
    "                 [0,0,0,1]],np.float32)\n",
    "\n",
    "rz   = np.array([[1,0,0,0],\n",
    "                 [0,1,0,0],\n",
    "                 [0,0,1,0],\n",
    "                 [0,0,0,1]],np.float32)\n",
    "\n",
    "\n",
    "trans= np.array([[1,0,0,0],\n",
    "                 [0,1,0,0],\n",
    "                 [0,0,1,height+20],\n",
    "                 [0,0,0,1]],np.float32)\n",
    "\n",
    "\n",
    "\n",
    "proj3dto2d = np.array([ [width,0,img.shape[1],0],\n",
    "                        [0,height,img.shape[0],0],\n",
    "                        [0,0,1,0] ],np.float32)\n",
    "\n",
    "x = -40.0 #0\n",
    "y = 0.0\n",
    "z = 0.0 #0\n",
    "\n",
    "\n",
    "ax = float(x * (math.pi / 180.0)) #0\n",
    "ay = float(y * (math.pi / 180.0)) \n",
    "az = float(z * (math.pi / 180.0)) #0\n",
    "\n",
    "rx[1,1] = math.cos(ax) #0\n",
    "rx[1,2] = -math.sin(ax) #0\n",
    "rx[2,1] = math.sin(ax) #0\n",
    "rx[2,2] = math.cos(ax) #0\n",
    "\n",
    "ry[0,0] = math.cos(ay)\n",
    "ry[0,2] = -math.sin(ay)\n",
    "ry[2,0] = math.sin(ay)\n",
    "ry[2,2] = math.cos(ay)\n",
    "\n",
    "rz[0,0] = math.cos(az) #0\n",
    "rz[0,1] = -math.sin(az) #0\n",
    "rz[1,0] = math.sin(az) #0\n",
    "rz[1,1] = math.cos(az) #0\n",
    "\n",
    "r =rx.dot(ry).dot(rz) # if we remove the lines we put    r=ry\n",
    "final = proj3dto2d.dot(trans.dot(r.dot(proj2dto3d)))\n",
    "\n",
    "dst = cv2.warpPerspective(img, final,(img.shape[1],img.shape[0]),None,cv2.INTER_LINEAR\n",
    "                            ,cv2.BORDER_CONSTANT,min_color)\n",
    "\n",
    "\n",
    "cv2.imshow(\"dst\",dst)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"./dumn2.PNG\")\n",
    "\n",
    "mask_img = color_fliter(img, \"blue\")\n",
    "    \n",
    "contour, hierarchy = find_contours(mask_img)\n",
    "\n",
    "contour_list = list()\n",
    "\n",
    "xy_list = list()\n",
    "\n",
    "# img_gray = cv.cvtColor(mask_img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# img_gray = cv.Canny(img_gray, 255, 255)\n",
    "\n",
    "# img_gray = np.where(img_gray > 200 , 255, img_gray)\n",
    "\n",
    "for pos in range(len(contour)):\n",
    "    \n",
    "    x, y, w, h = cv.boundingRect(contour[pos])\n",
    "\n",
    "    if (w > 20 and w < 100 and h > 40 and h < 150) or (h > 20 and h < 100 and w > 40 and w < 150):\n",
    "\n",
    "        # contour_list.append(contour[pos])\n",
    "\n",
    "        # part = img_gray[y+int(h*0.1):y+int(h*0.9), x+int(w*0.1):x+int(w*0.9)]\n",
    "        part = img[y:y+h, x:x+w]\n",
    "\n",
    "        # part = cv.Canny(part, 255, 255)\n",
    "\n",
    "        part = cv.resize(part, (zsize, zsize))\n",
    "\n",
    "        contour_list.append(part)\n",
    "        xy_list.append([x,y,w,h])\n",
    "\n",
    "        # show_img(part)\n",
    "\n",
    "k = im_model.predict(np.array(contour_list), verbose=0)\n",
    "\n",
    "k = np.argmax(k, axis=1)\n",
    "\n",
    "\n",
    "# img_gray = cv.cvtColor(img_gray, cv.COLOR_GRAY2BGR)\n",
    "\n",
    "for ((x,y,w,h), label) in zip(xy_list,k):\n",
    "\n",
    "    cv.rectangle(img, (x,y), (x+w, y+h), green, 3)\n",
    "    cv.putText(img, label_dict[label], (x, y), cv.FONT_HERSHEY_PLAIN, 2, red, 1, cv.LINE_AA)\n",
    "\n",
    "show_img(img)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#git test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53c7ce644e3f6210fe490b2e7844b26f537b2740ab8b6d88e5c764ce633e4d3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
